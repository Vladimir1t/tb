# routers/projects.py
from fastapi import APIRouter, HTTPException, Query, Request, Depends
from typing import List, Optional, Dict, Set
import sqlite3
import base64
import re
import math
from collections import Counter
from models import Project
from database_connect import get_db_connection
from .synonyms import SYNONYM_DB
from auth import verify_telegram_auth

router = APIRouter()

# –ö—ç—à –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
search_index = {}
project_data_cache = {}

def levenshtein_distance(s1: str, s2: str) -> int:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def find_similar_words_in_index(query_word: str, project_tokens: Set[str], max_distance: int = 2) -> List[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞"""
    similar_words = []
    for token in project_tokens:
        distance = levenshtein_distance(query_word, token)
        if distance <= max_distance and distance > 0:  # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–µ–ø–µ–Ω—å –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (1 - normalized_distance)
            max_len = max(len(query_word), len(token))
            similarity = 1 - (distance / max_len)
            similar_words.append((token, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
    similar_words.sort(key=lambda x: x[1], reverse=True)
    return [word for word, score in similar_words[:3]]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-3 –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤

def expand_with_synonyms(word: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç —Å–ª–æ–≤–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    synonyms = set()
    word_lower = word.lower()
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º–æ —Å–ª–æ–≤–æ
    synonyms.add(word_lower)
    
    # –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –≤ –±–∞–∑–µ
    if word_lower in SYNONYM_DB:
        synonyms.update(SYNONYM_DB[word_lower])
    
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—Å–Ω–æ–≤—ã —Å–ª–æ–≤
    for key, synonym_list in SYNONYM_DB.items():
        if word_lower in synonym_list:
            synonyms.add(key)
            synonyms.update(synonym_list)
    
    return synonyms

def expand_query_with_synonyms(query: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
    words = re.findall(r'\b\w{2,}\b', query.lower())
    expanded_terms = set()
    
    for word in words:
        synonyms = expand_with_synonyms(word)
        expanded_terms.update(synonyms)
    
    # print(f"üî§ Query expansion: '{query}' ‚Üí {expanded_terms}")
    return expanded_terms

# routers/projects.py

def build_search_index(conn):
    """–°—Ç—Ä–æ–∏–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
    global search_index, project_data_cache
    
    cursor = conn.cursor()
    cursor.execute("SELECT id, name, theme, type FROM projects")
    rows = cursor.fetchall()
    
    search_index = {}
    project_data_cache = {}
    
    # print(f"üîç Building search index for {len(rows)} projects")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
    all_unique_tokens = set()
    
    for row in rows:
        project = dict(row)
        project_id = project['id']
        project_data_cache[project_id] = project
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        content = f"{project['name']} {project['theme']} {project['type']}".lower()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = re.findall(r'\b\w{2,}\b', content)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –≤ –∏–Ω–¥–µ–∫—Å
        enhanced_tokens = set()
        for word in words:
            enhanced_tokens.add(word)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
            synonyms = expand_with_synonyms(word)
            enhanced_tokens.update(synonyms)
            
            # –î–æ–±–∞–≤–ª—è–µ–º n-grams –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            if len(word) > 3:
                for i in range(len(word) - 2):
                    enhanced_tokens.add(word[i:i+3])
        
        word_count = Counter(enhanced_tokens)
        total_words = len(enhanced_tokens)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º TF (Term Frequency)
        search_index[project_id] = {
            'tf': {word: count/total_words for word, count in word_count.items()},
            'content': content,
            'original_words': words,
            'all_tokens': set(enhanced_tokens)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
        }
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        all_unique_tokens.update(enhanced_tokens)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    global ALL_TOKENS
    ALL_TOKENS = list(all_unique_tokens)
    
    # print(f"üìä Search index built with {len(ALL_TOKENS)} unique tokens")

def find_similar_words_fast(query_word: str, max_distance: int = 2) -> List[str]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–∞"""
    similar_words = []
    query_lower = query_word.lower()
    
    for token in ALL_TOKENS:
        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –ø–µ—Ä–µ–¥ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        if abs(len(query_lower) - len(token)) > max_distance:
            continue
            
        distance = levenshtein_distance(query_lower, token)
        if distance <= max_distance and distance > 0:  # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            max_len = max(len(query_lower), len(token))
            similarity = 1 - (distance / max_len)
            similar_words.append((token, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
    similar_words.sort(key=lambda x: x[1], reverse=True)
    return [word for word, score in similar_words[:3]]

def spell_aware_semantic_search(query, threshold=0.01, top_k=30):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —É–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫"""
    global search_index
    
    # print(f"üîç Starting optimized spell-aware search for: '{query}'")
    # print(f"üîç Threshold: {threshold}, Top K: {top_k}")
    
    if not search_index:
        # print("‚ùå Search index is empty!")
        return []
    
    # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
    expanded_terms = expand_query_with_synonyms(query)
    
    if not expanded_terms:
        # print("‚ùå No valid terms after expansion")
        return []
    
    # print(f"üîç Expanded terms: {len(expanded_terms)} terms")
    
    # –¢–æ–∫–µ–Ω—ã –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    original_query_words = re.findall(r'\b\w{2,}\b', query.lower())
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
    similar_words_cache = {}
    for query_word in original_query_words:
        similar_words = find_similar_words_fast(query_word)
        if similar_words:
            similar_words_cache[query_word] = similar_words
            # print(f"   üìù Found similar words for '{query_word}': {similar_words}")
    
    # –°–æ–∑–¥–∞–µ–º TF –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    query_tf = {term: 1.0/len(expanded_terms) for term in expanded_terms}
    
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        project_tokens = project_data['all_tokens']
        
        # –°–ø–æ—Å–æ–± 1: –ë—ã—Å—Ç—Ä—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        for term in expanded_terms:
            if term in project_name:
                # –ü–æ–ª–Ω–æ–µ —Å–ª–æ–≤–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π score
                if any(term == word for word in project_data['original_words']):
                    similarity += 2.0
                else:
                    similarity += 1.5
            
            if term in project_theme:
                if any(term == word for word in project_data['original_words']):
                    similarity += 1.5
                else:
                    similarity += 1.0
        
        # –°–ø–æ—Å–æ–± 2: –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ (–æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏)
        for query_word, similar_words in similar_words_cache.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö –ø—Ä–æ–µ–∫—Ç–∞
            matched_similar = set(similar_words) & project_tokens
            if matched_similar:
                # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–Ω—É—Å –∑–∞ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
                similarity += 0.8 * len(matched_similar)
        
        # –°–ø–æ—Å–æ–± 3: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
        similarity = max(similarity, cosine_sim)
        
        # –°–ø–æ—Å–æ–± 4: –ë–æ–Ω—É—Å –∑–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
        for orig_word in original_query_words:
            if orig_word in project_name:
                similarity += 0.5
            if orig_word in project_theme:
                similarity += 0.3
        
        if similarity >= threshold:
            similarities.append((project_id, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –æ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    for pid, score in similarities[:3]:
        project_info = project_data_cache.get(pid, {})
        # print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}' - Score: {score:.4f}")
    
    return [{'id': pid, 'score': score} for pid, score in similarities[:top_k]]

# –î–æ–±–∞–≤–∏–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤
ALL_TOKENS = []

def calculate_cosine_similarity(query_tf, doc_tf):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
    all_words = set(query_tf.keys()) | set(doc_tf.keys())
    
    dot_product = 0
    query_magnitude = 0
    doc_magnitude = 0
    
    for word in all_words:
        query_val = query_tf.get(word, 0)
        doc_val = doc_tf.get(word, 0)
        
        dot_product += query_val * doc_val
        query_magnitude += query_val ** 2
        doc_magnitude += doc_val ** 2
    
    if query_magnitude == 0 or doc_magnitude == 0:
        return 0
    
    return dot_product / (math.sqrt(query_magnitude) * math.sqrt(doc_magnitude))

def normalize_search_term(term):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    if not term:
        return ""
    
    term = re.sub(r'\s+', ' ', term.lower()).strip()
    return term

@router.get("/projects/", response_model=List[Project])
async def get_projects(
    type: Optional[str] = None,
    theme: Optional[str] = None,
    search: Optional[str] = None,
    smart_search: Optional[str] = None,
    use_synonyms: bool = Query(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º"),
    spell_check: bool = Query(True, description="–ò—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"),  # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
    similarity_threshold: float = Query(0.01, ge=0.0, le=1.0),
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    conn = None
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        
        # print(f"üéØ RECEIVED REQUEST: type={type}, theme={theme}, search={search}, smart_search={smart_search}, use_synonyms={use_synonyms}, spell_check={spell_check}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
        if not search_index:
            print("üîÑ Building search index...")
            build_search_index(conn)
        else:
            print(f"‚úÖ Search index ready with {len(search_index)} projects")
        
        # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–≥–æ LIKE
        def ilike(pattern, value):
            if pattern is None or value is None:
                return False
            pattern_regex = pattern.replace('%', '.*').replace('_', '.')
            return bool(re.match(f"^{pattern_regex}$", value, re.IGNORECASE))
        
        conn.create_function("ilike", 2, ilike)
        cursor = conn.cursor()
        
        query = "SELECT * FROM projects WHERE 1=1"
        params = []

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        semantic_ids = []
        fallback_search = False
        
        if smart_search:
            # print(f"üéØ SMART SEARCH ACTIVATED: '{smart_search}'")
            normalized_search = normalize_search_term(smart_search)
            # print(f"üéØ Use synonyms: {use_synonyms}, Spell check: {spell_check}")
            
            if use_synonyms or spell_check:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –æ—à–∏–±–æ–∫
                semantic_results = spell_aware_semantic_search(normalized_search, similarity_threshold, limit * 5)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
                semantic_results = enhanced_semantic_search(normalized_search, similarity_threshold, limit * 5)
            
            # print(f"üéØ Semantic results: {len(semantic_results)} projects found")
            
            semantic_ids = [result['id'] for result in semantic_results]
            
            if semantic_ids:
                # print(f"‚úÖ Found {len(semantic_ids)} projects via semantic search")
                placeholders = ','.join('?' * len(semantic_ids))
                query += f" AND id IN ({placeholders})"
                params.extend(semantic_ids)
            else:
                # print("‚ùå No results from semantic search, using fallback...")
                fallback_search = True
                query += " AND (ilike(?, name) OR ilike(?, theme))"
                like_pattern = f"%{smart_search}%"
                params.extend([like_pattern, like_pattern])

        # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
        elif search:
            # print(f"üîç REGULAR SEARCH: '{search}'")
            query += " AND (ilike(?, name) OR ilike(?, theme) OR ilike(?, type))"
            like_pattern = f"%{search}%"
            params.extend([like_pattern, like_pattern, like_pattern])

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É
        if type:
            type_mapping = {'channels': 'channel', 'bots': 'bot', 'apps': 'mini_app'}
            normalized_type = type_mapping.get(type.lower(), type.lower())
            query += " AND ilike(?, type)"
            params.append(normalized_type)
            # print(f"üîß Type filter: {normalized_type}")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ
        if theme:
            query += " AND (ilike(?, name) OR ilike(?, theme))"
            like_pattern = f"%{theme}%"
            params.extend([like_pattern, like_pattern])
            # print(f"üîß Theme filter: {theme}")

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        if smart_search and semantic_ids and not fallback_search:
            order_case = "CASE "
            for i, project_id in enumerate(semantic_ids):
                order_case += f"WHEN id = {project_id} THEN {i} "
            order_case += f"ELSE {len(semantic_ids)} END"
            query += f" ORDER BY {order_case}, is_premium DESC, likes DESC"
            # print(f"üìä Sorting by semantic relevance")
        else:
            query += " ORDER BY is_premium DESC, likes DESC"
            # print(f"üìä Sorting by premium & likes")

        query += " LIMIT ? OFFSET ?"
        params.extend([limit, offset])

        # print(f"üìù Final SQL: {query}")
        # print(f"üìù Params count: {len(params)}")

        cursor.execute(query, params)
        rows = cursor.fetchall()

        projects = []
        for row in rows:
            project_data = dict(row)
            if project_data.get("icon"):
                project_data["icon"] = f"data:image/png;base64,{base64.b64encode(project_data['icon']).decode()}"
            else:
                project_data["icon"] = None
            projects.append(project_data)
        
        # print(f"‚úÖ Returning {len(projects)} projects")
        return projects
        
    except sqlite3.Error as e:
        # print(f"‚ùå SQL error: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ SQL-–∑–∞–ø—Ä–æ—Å–∞: {e}")
    finally:
        if conn:
            conn.close()

def enhanced_semantic_search(query, threshold=0.01, top_k=20):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    global search_index
    
    # print(f"üîç Starting enhanced search for: '{query}'")
    # print(f"üîç Threshold: {threshold}, Top K: {top_k}")
    
    if not search_index:
        # print("‚ùå Search index is empty!")
        return []
    
    query_lower = query.lower()
    query_words = re.findall(r'\b\w{2,}\b', query_lower)
    
    if not query_words:
        # print("‚ùå No valid words in query")
        return []
    
    # print(f"üîç Query words: {query_words}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        
        # –°–ø–æ—Å–æ–± 1: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        partial_matches = find_partial_matches(query_lower, project_data['tf'])
        if partial_matches:
            best_match_score = max(score for _, score in partial_matches)
            similarity = max(similarity, best_match_score)
        
        # –°–ø–æ—Å–æ–± 2: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Å–ª–æ–≤–∞–º
        if query_words:
            query_tf = {word: 1.0/len(query_words) for word in query_words}
            cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
            similarity = max(similarity, cosine_sim)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º score –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤, –≥–¥–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –≤ –Ω–∞–∑–≤–∞–Ω–∏–µ
        for q_word in query_words:
            if q_word in project_name:
                similarity += 0.3
            if q_word in project_theme:
                similarity += 0.2
        
        if similarity >= threshold:
            similarities.append((project_id, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    # print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
    for pid, score in similarities[:5]:
        project_info = project_data_cache.get(pid, {})
        # print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}'")
        # print(f"      Theme: {project_info.get('theme', 'N/A')}")
        # print(f"      Score: {score:.4f}")
    
    return [{'id': pid, 'score': score} for pid, score in similarities[:top_k]]





from fastapi import APIRouter, HTTPException, Query, Request, Depends
from typing import List, Optional, Dict, Set, Tuple
import sqlite3
import base64
import re
import math
from collections import Counter
import threading
from models import Project
from database_connect import get_db_connection
from .synonyms import SYNONYM_DB
from auth import verify_telegram_auth
from functools import lru_cache
import time
import hashlib
import gc
import psutil
import os
import random 
from datetime import datetime, timedelta

router = APIRouter()

search_index = {}
project_data_cache = {}
ALL_TOKENS = []
_index_lock = threading.Lock()

_search_cache = {}
_cache_lock = threading.Lock()
_CACHE_TTL = 300

class SearchCache:
    def __init__(self, max_size=500, ttl=300):
        self._cache = {}
        self._max_size = max_size
        self._ttl = ttl
        self._lock = threading.Lock()
    
    def get(self, key):
        with self._lock:
            if key in self._cache:
                data, timestamp = self._cache[key]
                if time.time() - timestamp < self._ttl:
                    return data
                else:
                    del self._cache[key]
            return None
    
    def set(self, key, data):
        with self._lock:
            if len(self._cache) >= self._max_size:
                self._cleanup()
            self._cache[key] = (data, time.time())
    
    def _cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self._cache.items()
            if current_time - timestamp > self._ttl
        ]
        for key in expired_keys:
            del self._cache[key]
        
        if len(self._cache) >= self._max_size:
            sorted_keys = sorted(
                self._cache.keys(), 
                key=lambda k: self._cache[k][1]
            )
            for key in sorted_keys[:self._max_size // 2]:
                del self._cache[key]
    
    def clear(self):
        with self._lock:
            self._cache.clear()
    
    def size(self):
        with self._lock:
            return len(self._cache)

# –ó–ê–ú–ï–ù–ê –ø—Ä–æ—Å—Ç–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ —É–º–Ω—ã–π –∫—ç—à
_search_cache = SearchCache(max_size=500, ttl=300)

def get_search_cache_key(query: str, params: dict) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞ –ø–æ–∏—Å–∫–∞"""
    key_data = f"{query}_{params}"
    return hashlib.md5(key_data.encode()).hexdigest()

def clear_memory():
    """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
    gc.collect()

    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ–∏—Å–∫–∞ –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
    if _search_cache.size() > 1000:
        print(f"üßπ Clearing large cache: {_search_cache.size()} entries")
        _search_cache.clear()

@lru_cache(maxsize=100)
def perform_search_once(query: str, use_synonyms: bool, spell_check: bool, threshold: float) -> tuple:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"""
    print(f"üîç Performing search for: '{query}'")
    
    if len(query) > 100:
        query = query[:100]
    
    normalized_search = normalize_search_term(query)
    search_limit = 500 if use_synonyms or spell_check else 200
    
    if use_synonyms or spell_check:
        semantic_results = spell_aware_semantic_search(normalized_search, threshold, search_limit)
    else:
        semantic_results = enhanced_semantic_search(normalized_search, threshold, search_limit)
    
    semantic_ids = [result['id'] for result in semantic_results]
    print(f"‚úÖ Found {len(semantic_ids)} total projects via semantic search")
    
    return tuple(semantic_ids)

@router.get("/projects/", response_model=List[Project])
async def get_projects(
    type: Optional[str] = None,
    theme: Optional[str] = None,
    search: Optional[str] = None,
    smart_search: Optional[str] = None,
    use_synonyms: bool = Query(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º"),
    spell_check: bool = Query(True, description="–ò—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"),  
    similarity_threshold: float = Query(0.01, ge=0.0, le=1.0),
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    conn = None
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        
        with _index_lock:
            if not search_index:
                print("üîÑ Building search index...")
                build_search_index(conn)
            else:
                print(f"‚úÖ Use ready Search index for {len(search_index)} projects")
        
        def ilike(pattern, value):
            if pattern is None or value is None:
                return False
            pattern_regex = pattern.replace('%', '.*').replace('_', '.')
            return bool(re.match(f"^{pattern_regex}$", value, re.IGNORECASE))
        
        conn.create_function("ilike", 2, ilike)
        cursor = conn.cursor()
        
        query = "SELECT * FROM projects WHERE 1=1"
        params = []

        semantic_ids_smart = []
        semantic_ids_theme = []
        fallback_smart_search = False
        fallback_theme_search = False
        
        # –û–ë–†–ê–ë–û–¢–ö–ê SMART_SEARCH (–ø–æ –≤—Å–µ–º –ø–æ–ª—è–º)
        if smart_search:
            print(f"====== üîç Smart search: '{smart_search}' ======")
            
            cache_key_smart = f"smart_{smart_search}_{use_synonyms}_{spell_check}_{similarity_threshold}"
            cached_result = _search_cache.get(cache_key_smart)
            
            if cached_result is None:
                semantic_results_smart = perform_search_once(smart_search, use_synonyms, spell_check, similarity_threshold)
                _search_cache.set(cache_key_smart, semantic_results_smart)
            else:
                semantic_results_smart = cached_result
                print(f"üì¶ Using cached smart search results ({len(semantic_results_smart)} items)")
            
            semantic_ids_smart = list(semantic_results_smart)
            
            if not semantic_ids_smart:
                print("‚ùå No results from smart search, using fallback...")
                fallback_smart_search = True
                query += " AND (ilike(?, name) OR ilike(?, theme) OR ilike(?, type))"
                like_pattern = f"%{smart_search}%"
                params.extend([like_pattern, like_pattern, like_pattern])
        
        # –û–ë–†–ê–ë–û–¢–ö–ê THEME (—Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–º–∞–º —Å smart search)
        if theme:
            print(f"====== üé® Theme smart search: '{theme}' ======")
            
            cache_key_theme = f"theme_{theme}_{use_synonyms}_{spell_check}_{similarity_threshold}"
            cached_result = _search_cache.get(cache_key_theme)
            
            if cached_result is None:
                semantic_results_theme = perform_search_once(theme, use_synonyms, spell_check, similarity_threshold)
                _search_cache.set(cache_key_theme, semantic_results_theme)
            else:
                semantic_results_theme = cached_result
                print(f"Using cached theme search results ({len(semantic_results_theme)} items)")
            
            semantic_ids_theme = list(semantic_results_theme)
            
            if not semantic_ids_theme:
                print("‚ùå No results from theme search, using fallback...")
                fallback_theme_search = True
                query += " AND ilike(?, theme)"
                like_pattern = f"%{theme}%"
                params.append(like_pattern)
        
        # –û–ë–†–ê–ë–û–¢–ö–ê REGULAR SEARCH
        elif search:
            #print(f"üîç Regular search: '{search}'")
            query += " AND (ilike(?, name) OR ilike(?, theme) OR ilike(?, type))"
            like_pattern = f"%{search}%"
            params.extend([like_pattern, like_pattern, like_pattern])

        # –ü–†–ò–ú–ï–ù–ï–ù–ò–ï SEMANTIC IDS –ï–°–õ–ò –ï–°–¢–¨ –†–ï–ó–£–õ–¨–¢–ê–¢–´
        semantic_ids_to_use = []
        
        if semantic_ids_smart and semantic_ids_theme:
            semantic_ids_to_use = list(set(semantic_ids_smart) & set(semantic_ids_theme))
            print(f"===== üéØ Combined smart + theme search: {len(semantic_ids_smart)} smart ‚à© {len(semantic_ids_theme)} theme = {len(semantic_ids_to_use)} projects")
            
        elif semantic_ids_smart:
            semantic_ids_to_use = semantic_ids_smart
            print(f"===== üéØ Using smart search results: {len(semantic_ids_to_use)} projects =====")
            
        elif semantic_ids_theme:
            semantic_ids_to_use = semantic_ids_theme
            print(f"===== üéØ Using theme search results: {len(semantic_ids_to_use)} projects =====")
        
        # –î–û–ë–ê–í–õ–Ø–ï–ú SEMANTIC IDS –í –ó–ê–ü–†–û–°
        if semantic_ids_to_use and type:
            type_mapping = {'channels': 'channel', 'bots': 'bot', 'apps': 'mini_app'}
            normalized_type = type_mapping.get(type.lower(), type.lower())
            
            filtered_semantic_ids = []
            for project_id in semantic_ids_to_use:
                project_info = project_data_cache.get(project_id, {})
                if project_info.get('type', '').lower() == normalized_type:
                    filtered_semantic_ids.append(project_id)
            
            print(f"üîç After type filter: {len(filtered_semantic_ids)}/{len(semantic_ids_to_use)} projects are {normalized_type}s")
            semantic_ids_to_use = filtered_semantic_ids

        # –¢–ï–ü–ï–†–¨ –ø—Ä–∏–º–µ–Ω—è–µ–º –ø–∞–≥–∏–Ω–∞—Ü–∏—é –∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º ID
        paginated_ids = []
        if semantic_ids_to_use:
            start_idx = offset
            end_idx = offset + limit
            paginated_ids = semantic_ids_to_use[start_idx:end_idx]
            
            if paginated_ids:
                placeholders = ','.join('?' * len(paginated_ids))
                query += f" AND id IN ({placeholders})"
                params.extend(paginated_ids)
                print(f"üìÑ Pagination: {start_idx}-{end_idx} of {len(semantic_ids_to_use)} {type} projects")
            else:
                query += " AND 1=0"
                print(f"‚ùå No {type} projects in paginated range")

        # –§–ò–õ–¨–¢–† –ü–û TYPE (–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        if type:
            type_mapping = {'channels': 'channel', 'bots': 'bot', 'apps': 'mini_app'}
            normalized_type = type_mapping.get(type.lower(), type.lower())
            query += " AND ilike(?, type)"
            params.append(normalized_type)

        # –°–û–†–¢–ò–†–û–í–ö–ê
        if semantic_ids_to_use and paginated_ids:
            order_case = "CASE "
            for i, project_id in enumerate(paginated_ids):
                order_case += f"WHEN id = {project_id} THEN {i} "
            order_case += f"ELSE {len(paginated_ids)} END"
            query += f" ORDER BY {order_case}"
        else:
            query += " ORDER BY is_premium DESC, likes DESC"

        # –ü–ê–ì–ò–ù–ê–¶–ò–Ø –¥–ª—è —Å–ª—É—á–∞–µ–≤ –±–µ–∑ semantic results –∏–ª–∏ –ø—Ä–∏ fallback
        if (not semantic_ids_to_use or not paginated_ids) and (not smart_search or fallback_smart_search) and (not theme or fallback_theme_search):
            query += " LIMIT ? OFFSET ?"
            params.extend([limit, offset])

       # print(f"üìù Executing query with {len(params)} params")

        cursor.execute(query, params)
        rows = cursor.fetchall()

        projects = []
        for row in rows:
            project_data = dict(row)
            if project_data.get("icon"):
                project_data["icon"] = f"data:image/png;base64,{base64.b64encode(project_data['icon']).decode()}"
            else:
                project_data["icon"] = None
            projects.append(project_data)
        
        print(f"‚úÖ Returning {len(projects)} projects")
        return projects
        
    except sqlite3.Error as e:
        print(f"‚ùå SQL error: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ SQL-–∑–∞–ø—Ä–æ—Å–∞: {e}")
    finally:
        if random.random() < 0.1:  # 10% chance —á—Ç–æ–±—ã –Ω–µ –∑–∞–º–µ–¥–ª—è—Ç—å
            clear_memory()
        if conn:
            conn.close()

@router.post("/memory-status")
async def memory_status():
    """–°—Ç–∞—Ç—É—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        "rss_mb": round(memory_info.rss / 1024 / 1024, 2),
        "vms_mb": round(memory_info.vms / 1024 / 1024, 2),
        "search_index_size": len(search_index),
        "project_cache_size": len(project_data_cache),
        "tokens_count": len(ALL_TOKENS),
        "search_cache_size": _search_cache.size()
    }

@router.post("/clear-search-cache")
async def clear_search_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞ –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç –ø–∞–º—è—Ç—å"""
    perform_search_once.cache_clear()
    _search_cache.clear()    
    gc.collect()
    
    return {
        "message": "Search cache cleared and memory freed",
        "memory_status": await memory_status()
    }

def find_partial_matches(query: str, doc_tf: Dict[str, float]) -> List[Tuple[str, float]]:
    """–ù–∞—Ö–æ–¥–∏—Ç —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    matches = []
    query_terms = query.lower().split()
    
    for term in query_terms:
        if len(term) < 2:
            continue
            
        for doc_term, score in doc_tf.items():
            if term in doc_term or doc_term in term:
                similarity = min(len(term), len(doc_term)) / max(len(term), len(doc_term))
                if similarity > 0.6:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                    matches.append((doc_term, score * similarity))
    
    return matches

def levenshtein_distance(s1: str, s2: str) -> int:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def stem_word(word: str) -> str:
    """–ë–∞–∑–æ–≤–∞—è —Å—Ç–µ–º–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤"""
    if not word or len(word) < 3:
        return word
    
    russian_endings = [
        '–æ–≤', '–µ–≤', '–∏–Ω', '—ã–Ω', '—ã—Ö', '–∏—Ö', '–æ–µ', '–µ–µ', '—ã–µ', '–∏–µ', '–æ–º—É', '–µ–º—É', 
        '—ã–º–∏', '–∏–º–∏', '–æ–º', '–µ–º', '–∞—Ö', '—è—Ö', '–∞–º–∏', '—è–º–∏', '—É—é', '—é—é', '–µ–π', '–æ–π',
        '–∞', '—è', '–æ', '–µ', '–∏', '—ã', '—É', '—é', '—å', '–π', '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏'
    ]
    
    english_endings = [
        'ing', 'ed', 'es', 's', 'ly', 'er', 'est', 'ment', 'ness', 'tion', 'sion'
    ]
    
    for ending in english_endings:
        if word.endswith(ending) and len(word) > len(ending) + 2:
            return word[:-len(ending)]
    
    for ending in russian_endings:
        if word.endswith(ending) and len(word) > len(ending) + 2:
            return word[:-len(ending)]
    
    return word

def normalize_and_stem(text: str) -> Set[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç–µ–º–º–∏–Ω–≥–æ–º"""
    words = re.findall(r'\b\w{2,}\b', text.lower())
    stemmed_words = set()
    
    for word in words:
        stemmed_words.add(word)
        stemmed = stem_word(word)
        if stemmed != word and len(stemmed) >= 2:
            stemmed_words.add(stemmed)
    
    return stemmed_words

def expand_with_synonyms(word: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç —Å–ª–æ–≤–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
    synonyms = set()
    word_lower = word.lower()
    
    synonyms.add(word_lower)
    
    stemmed = stem_word(word_lower)
    if stemmed != word_lower:
        synonyms.add(stemmed)
    
    # –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –≤ –±–∞–∑–µ
    if word_lower in SYNONYM_DB:
        synonyms.update(SYNONYM_DB[word_lower])
    
    if stemmed in SYNONYM_DB:
        synonyms.update(SYNONYM_DB[stemmed])
    
    for key, synonym_list in SYNONYM_DB.items():
        if word_lower in synonym_list or stemmed in synonym_list:
            synonyms.add(key)
            synonyms.update(synonym_list)
    
    return synonyms

def expand_query_with_synonyms(query: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
    words = re.findall(r'\b\w{2,}\b', query.lower())
    expanded_terms = set()
    
    for word in words:
        synonyms = expand_with_synonyms(word)
        expanded_terms.update(synonyms)
    
    return expanded_terms

def build_search_index(conn):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π –ø–∞–º—è—Ç–∏"""
    global search_index, project_data_cache, ALL_TOKENS
    
    with _index_lock:
        print(f"üîÑ Building optimized search index...")
        
        # –û–ß–ò–°–¢–ö–ê –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –°–û–ó–î–ê–ù–ò–ï–ú –ù–û–í–û–ì–û –ò–ù–î–ï–ö–°–ê
        old_index_size = len(search_index)
        old_cache_size = len(project_data_cache)
        old_tokens_size = len(ALL_TOKENS)
        
        # –Ø–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        search_index.clear()
        project_data_cache.clear()
        ALL_TOKENS.clear()
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        gc.collect()
        
        print(f"üßπ Cleared: index={old_index_size}, cache={old_cache_size}, tokens={old_tokens_size}")
        
        # –ù–û–í–û–ï: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        cursor = conn.cursor()
        cursor.execute("SELECT id, name, theme, type, is_premium FROM projects")
        rows = cursor.fetchall()
        
        all_unique_tokens = set()
        processed_count = 0
        
        for row in rows:
            project = dict(row)
            project_id = project['id']
            project_data_cache[project_id] = project
            
            content = f"{project['name']} {project['theme']} {project['type']}".lower()
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –ø—Ä–æ–µ–∫—Ç
            stemmed_words = normalize_and_stem(content)
            enhanced_tokens = set()
            
            for word in list(stemmed_words)[:50]:  # –ú–∞–∫—Å–∏–º—É–º 50 —Å–ª–æ–≤ –Ω–∞ –ø—Ä–æ–µ–∫—Ç
                enhanced_tokens.add(word)
                synonyms = expand_with_synonyms(word)
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
                enhanced_tokens.update(list(synonyms)[:10])
                
                if len(word) > 3:
                    for i in range(min(len(word) - 2, 5)):  # –ú–∞–∫—Å–∏–º—É–º 5 n-gram
                        enhanced_tokens.add(word[i:i+3])
            
            word_count = Counter(enhanced_tokens)
            total_words = len(enhanced_tokens)
            
            search_index[project_id] = {
                'tf': {word: count/total_words for word, count in word_count.items()},
                'content': content,
                'original_words': stemmed_words,
                'all_tokens': set(enhanced_tokens),
                'is_premium': project.get('is_premium', False)
            }
            
            all_unique_tokens.update(enhanced_tokens)
            processed_count += 1
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–∑
            if processed_count % 100 == 0:
                gc.collect()
        
        ALL_TOKENS = list(all_unique_tokens)
        print(f"üìä Optimized index built: {len(ALL_TOKENS)} tokens for {processed_count} projects")
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à –ø–æ–∏—Å–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞
        clear_search_cache()

def find_similar_words_fast(query_word: str, max_distance: int = 2) -> List[str]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–∞"""
    similar_words = []
    query_lower = query_word.lower()
    query_stemmed = stem_word(query_lower)
    
    for token in ALL_TOKENS:
        if abs(len(query_lower) - len(token)) > max_distance:
            continue
            
        distance = levenshtein_distance(query_lower, token)
        if distance <= max_distance and distance > 0:  
            max_len = max(len(query_lower), len(token))
            similarity = 1 - (distance / max_len)
            similar_words.append((token, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
    similar_words.sort(key=lambda x: x[1], reverse=True)
    return [word for word, score in similar_words[:3]]

def spell_aware_semantic_search(query, threshold=0.2, top_k=30):
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ–º –ø–æ score"""
    global search_index
    
   # print(f"üîç Starting spell-aware search for: '{query}'")
    
    if not search_index:
        print("‚ùå Search index is empty!")
        return []
    
    expanded_terms = expand_query_with_synonyms(query)
    
    if not expanded_terms:
        print("‚ùå No valid terms after expansion")
        return []
    
    original_query_words = re.findall(r'\b\w{2,}\b', query.lower())
    
    use_detailed_spell_check = len(original_query_words) <= 3
    
    similar_words_cache = {}
    if use_detailed_spell_check:
        for query_word in original_query_words:
            similar_words = find_similar_words_fast(query_word)
            if similar_words:
                similar_words_cache[query_word] = similar_words
    
    query_tf = {term: 1.0/len(expanded_terms) for term in expanded_terms}
    
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        project_tokens = project_data['all_tokens']
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        exact_name_matches = 0
        for term in expanded_terms:
            if term in project_name and any(term == word for word in project_data['original_words']):
                exact_name_matches += 1
        if exact_name_matches > 0:
            similarity += exact_name_matches * 3.0  
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–º–µ
        exact_theme_matches = 0
        for term in expanded_terms:
            if term in project_theme and any(term == word for word in project_data['original_words']):
                exact_theme_matches += 1
        if exact_theme_matches > 0:
            similarity += exact_theme_matches * 2.0  
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        partial_name_matches = 0
        for term in expanded_terms:
            if term in project_name and not any(term == word for word in project_data['original_words']):
                partial_name_matches += 1
        if partial_name_matches > 0:
            similarity += partial_name_matches * 1.5
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 4: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–º–µ
        partial_theme_matches = 0
        for term in expanded_terms:
            if term in project_theme and not any(term == word for word in project_data['original_words']):
                partial_theme_matches += 1
        if partial_theme_matches > 0:
            similarity += partial_theme_matches * 1.0
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 5: –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
        if use_detailed_spell_check and similar_words_cache:
            similar_word_bonus = 0
            for query_word, similar_words in similar_words_cache.items():
                matched_similar = set(similar_words) & project_tokens
                if matched_similar:
                    similar_word_bonus += min(0.5, 0.2 * len(matched_similar))
            similarity += similar_word_bonus
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 6: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
        similarity += min(cosine_sim, 1.0)
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–µ–º–∏—É–º –ø—Ä–æ–µ–∫—Ç—ã
        if project_info.get('is_premium'):
            similarity += 0.1
        
        if similarity >= threshold:
            similarities.append((project_id, similarity, exact_name_matches, exact_theme_matches))

    # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è
    if similarities:
        similarities.sort(key=lambda x: x[1], reverse=True)
        scores = [score for _, score, _, _ in similarities]
        
        if len(scores) > 2:
            top_score = scores[0]
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥: –º–∏–Ω–∏–º—É–º 60% –æ—Ç –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            dynamic_threshold = max(threshold, top_score * 0.60)
            absolute_min_threshold = 0.4  
            final_threshold = min(dynamic_threshold, absolute_min_threshold)
            
            # print(f"üéØ Dynamic threshold: {final_threshold:.3f}")
            
            filtered_count_before = len(similarities)
            similarities = [
                (pid, score, name_m, theme_m) 
                for pid, score, name_m, theme_m in similarities 
                if score >= final_threshold
            ]
            filtered_count_after = len(similarities)
            
            print(f"üìä Filtered: {filtered_count_before} ‚Üí {filtered_count_after} results")
    
   # print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    for pid, score, name_matches, theme_matches in similarities[:5]:
        project_info = project_data_cache.get(pid, {})
        print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}'")
        print(f"      Theme: {project_info.get('theme', 'N/A')}")
        print(f"      Score: {score:.4f} (name_matches: {name_matches}, theme_matches: {theme_matches})")
    
    return [{'id': pid, 'score': score} for pid, score, _, _ in similarities[:top_k]]

ALL_TOKENS = []

def calculate_cosine_similarity(query_tf, doc_tf):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
    all_words = set(query_tf.keys()) | set(doc_tf.keys())
    
    dot_product = 0
    query_magnitude = 0
    doc_magnitude = 0
    
    for word in all_words:
        query_val = query_tf.get(word, 0)
        doc_val = doc_tf.get(word, 0)
        
        dot_product += query_val * doc_val
        query_magnitude += query_val ** 2
        doc_magnitude += doc_val ** 2
    
    if query_magnitude == 0 or doc_magnitude == 0:
        return 0
    
    return dot_product / (math.sqrt(query_magnitude) * math.sqrt(doc_magnitude))

def normalize_search_term(term):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    if not term:
        return ""
    
    term = re.sub(r'\s+', ' ', term.lower()).strip()
    return term

def enhanced_semantic_search(query, threshold=0.01, top_k=20):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    global search_index
    
   # print(f"üîç Starting enhanced search for: '{query}'")
   # print(f"üîç Threshold: {threshold}, Top K: {top_k}")
    
    if not search_index:
        print("‚ùå Search index is empty!")
        return []
    
    query_lower = query.lower()
    query_words = re.findall(r'\b\w{2,}\b', query_lower)
    
    if not query_words:
        print("‚ùå No valid words in query")
        return []
    
    # print(f"üîç Query words: {query_words}")
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        
        # –°–ø–æ—Å–æ–± 1: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        partial_matches = find_partial_matches(query_lower, project_data['tf'])
        if partial_matches:
            best_match_score = max(score for _, score in partial_matches)
            similarity = max(similarity, best_match_score)
        
        # –°–ø–æ—Å–æ–± 2: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Å–ª–æ–≤–∞–º
        if query_words:
            query_tf = {word: 1.0/len(query_words) for word in query_words}
            cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
            similarity = max(similarity, cosine_sim)
        
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        
        for q_word in query_words:
            if q_word in project_name:
                similarity += 0.3
            if q_word in project_theme:
                similarity += 0.2
        
        if similarity >= threshold:
            similarities.append((project_id, similarity))
    
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    for pid, score in similarities[:5]:
        project_info = project_data_cache.get(pid, {})
        print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}'")
        print(f"      Theme: {project_info.get('theme', 'N/A')}")
        print(f"      Score: {score:.4f}")
    
    return [{'id': pid, 'score': score} for pid, score in similarities[:top_k]]

def refresh_search_index(conn):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    print("üîÑ Forced search index refresh...")
    build_search_index(conn)
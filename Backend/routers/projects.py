# routers/projects.py
from fastapi import APIRouter, HTTPException, Query, Request, Depends
from typing import List, Optional, Dict, Set, Tuple
import sqlite3
import base64
import re
import math
from collections import Counter
import threading
from models import Project
from database_connect import get_db_connection
from .synonyms import SYNONYM_DB
from auth import verify_telegram_auth
from functools import lru_cache
import time
import hashlib

router = APIRouter()

search_index = {}
project_data_cache = {}
ALL_TOKENS = []
_index_lock = threading.Lock()

_search_cache = {}
_cache_lock = threading.Lock()
_CACHE_TTL = 300  

def get_search_cache_key(query: str, params: dict) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –¥–ª—è –∫—ç—à–∞ –ø–æ–∏—Å–∫–∞"""
    key_data = f"{query}_{params}"
    return hashlib.md5(key_data.encode()).hexdigest()

@lru_cache(maxsize=100)
def perform_search_once(query: str, use_synonyms: bool, spell_check: bool, threshold: float) -> tuple:
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –æ–¥–∏–Ω —Ä–∞–∑ –∏ –∫—ç—à–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    print(f"üîç Performing search for: '{query}'")
    
    normalized_search = normalize_search_term(query)
    
    if use_synonyms or spell_check:
        semantic_results = spell_aware_semantic_search(normalized_search, threshold, 1000)  # –ë–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç
    else:
        semantic_results = enhanced_semantic_search(normalized_search, threshold, 1000)
    
    semantic_ids = [result['id'] for result in semantic_results]
    print(f"‚úÖ Found {len(semantic_ids)} total projects via semantic search")
    
    return tuple(semantic_ids)  

@router.get("/projects/", response_model=List[Project])
async def get_projects(
    type: Optional[str] = None,
    theme: Optional[str] = None,
    search: Optional[str] = None,
    smart_search: Optional[str] = None,
    use_synonyms: bool = Query(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º"),
    spell_check: bool = Query(True, description="–ò—Å–ø—Ä–∞–≤–ª—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏"),  
    similarity_threshold: float = Query(0.01, ge=0.0, le=1.0),
    limit: int = Query(10, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    conn = None
    try:
        conn = get_db_connection()
        conn.row_factory = sqlite3.Row
        
        with _index_lock:
            if not search_index:
                print("üîÑ Building search index...")
                build_search_index(conn)
            else:
                print(f"‚úÖ Search index ready with {len(search_index)} projects")
        
        def ilike(pattern, value):
            if pattern is None or value is None:
                return False
            pattern_regex = pattern.replace('%', '.*').replace('_', '.')
            return bool(re.match(f"^{pattern_regex}$", value, re.IGNORECASE))
        
        conn.create_function("ilike", 2, ilike)
        cursor = conn.cursor()
        
        query = "SELECT * FROM projects WHERE 1=1"
        params = []

        semantic_ids_smart = []
        semantic_ids_theme = []
        fallback_smart_search = False
        fallback_theme_search = False
        
        # –û–ë–†–ê–ë–û–¢–ö–ê SMART_SEARCH (–ø–æ –≤—Å–µ–º –ø–æ–ª—è–º)
        if smart_search:
            print(f"üîç Smart search: '{smart_search}'")
            
            cache_key_smart = f"smart_{smart_search}_{use_synonyms}_{spell_check}_{similarity_threshold}"
            if cache_key_smart not in _search_cache or time.time() - _search_cache[cache_key_smart]['timestamp'] > _CACHE_TTL:
                semantic_results_smart = perform_search_once(smart_search, use_synonyms, spell_check, similarity_threshold)
                with _cache_lock:
                    _search_cache[cache_key_smart] = {
                        'ids': semantic_results_smart,
                        'timestamp': time.time()
                    }
            else:
                semantic_results_smart = _search_cache[cache_key_smart]['ids']
                print(f"üì¶ Using cached smart search results ({len(semantic_results_smart)} items)")
            
            semantic_ids_smart = list(semantic_results_smart)
            
            if not semantic_ids_smart:
                print("‚ùå No results from smart search, using fallback...")
                fallback_smart_search = True
                query += " AND (ilike(?, name) OR ilike(?, theme) OR ilike(?, type))"
                like_pattern = f"%{smart_search}%"
                params.extend([like_pattern, like_pattern, like_pattern])
        
        # –û–ë–†–ê–ë–û–¢–ö–ê THEME (—Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–º–∞–º —Å smart search)
        if theme:
            print(f"üé® Theme smart search: '{theme}'")
            
            cache_key_theme = f"theme_{theme}_{use_synonyms}_{spell_check}_{similarity_threshold}"
            if cache_key_theme not in _search_cache or time.time() - _search_cache[cache_key_theme]['timestamp'] > _CACHE_TTL:
                semantic_results_theme = perform_search_once(theme, use_synonyms, spell_check, similarity_threshold)
                with _cache_lock:
                    _search_cache[cache_key_theme] = {
                        'ids': semantic_results_theme,
                        'timestamp': time.time()
                    }
            else:
                semantic_results_theme = _search_cache[cache_key_theme]['ids']
                print(f"üì¶ Using cached theme search results ({len(semantic_results_theme)} items)")
            
            semantic_ids_theme = list(semantic_results_theme)
            
            if not semantic_ids_theme:
                print("‚ùå No results from theme search, using fallback...")
                fallback_theme_search = True
                query += " AND ilike(?, theme)"
                like_pattern = f"%{theme}%"
                params.append(like_pattern)
        
        # –û–ë–†–ê–ë–û–¢–ö–ê REGULAR SEARCH
        elif search:
            print(f"üîç Regular search: '{search}'")
            query += " AND (ilike(?, name) OR ilike(?, theme) OR ilike(?, type))"
            like_pattern = f"%{search}%"
            params.extend([like_pattern, like_pattern, like_pattern])

        # –ü–†–ò–ú–ï–ù–ï–ù–ò–ï SEMANTIC IDS –ï–°–õ–ò –ï–°–¢–¨ –†–ï–ó–£–õ–¨–¢–ê–¢–´
        semantic_ids_to_use = []
        
        if semantic_ids_smart and semantic_ids_theme:
            # –ü–ï–†–ï–°–ï–ß–ï–ù–ò–ï: –ø—Ä–æ–µ–∫—Ç—ã –¥–æ–ª–∂–Ω—ã —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—Ç—å –û–ë–û–ò–ú —É—Å–ª–æ–≤–∏—è–º
            semantic_ids_to_use = list(set(semantic_ids_smart) & set(semantic_ids_theme))
            print(f"üéØ Combined smart + theme search: {len(semantic_ids_smart)} smart ‚à© {len(semantic_ids_theme)} theme = {len(semantic_ids_to_use)} projects")
            
        elif semantic_ids_smart:
            # –¢–æ–ª—å–∫–æ smart search
            semantic_ids_to_use = semantic_ids_smart
            print(f"üéØ Using smart search results: {len(semantic_ids_to_use)} projects")
            
        elif semantic_ids_theme:
            # –¢–æ–ª—å–∫–æ theme search
            semantic_ids_to_use = semantic_ids_theme
            print(f"üéØ Using theme search results: {len(semantic_ids_to_use)} projects")
        
        # –î–û–ë–ê–í–õ–Ø–ï–ú SEMANTIC IDS –í –ó–ê–ü–†–û–°
        if semantic_ids_to_use:
            start_idx = offset
            end_idx = offset + limit
            paginated_ids = semantic_ids_to_use[start_idx:end_idx]
            
            if paginated_ids:
                placeholders = ','.join('?' * len(paginated_ids))
                query += f" AND id IN ({placeholders})"
                params.extend(paginated_ids)
                print(f"üìÑ Pagination: {start_idx}-{end_idx} of {len(semantic_ids_to_use)}")
            else:
                query += " AND 1=0"

        # –§–ò–õ–¨–¢–† –ü–û TYPE (–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞ –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        if type:
            type_mapping = {'channels': 'channel', 'bots': 'bot', 'apps': 'mini_app'}
            normalized_type = type_mapping.get(type.lower(), type.lower())
            query += " AND ilike(?, type)"
            params.append(normalized_type)

        # –°–û–†–¢–ò–†–û–í–ö–ê
        if semantic_ids_to_use and paginated_ids:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–∞–≥–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö ID
            order_case = "CASE "
            for i, project_id in enumerate(paginated_ids):
                order_case += f"WHEN id = {project_id} THEN {i} "
            order_case += f"ELSE {len(paginated_ids)} END"
            query += f" ORDER BY {order_case}"
        else:
            query += " ORDER BY is_premium DESC, likes DESC"

        # –ü–ê–ì–ò–ù–ê–¶–ò–Ø –¥–ª—è —Å–ª—É—á–∞–µ–≤ –±–µ–∑ semantic results –∏–ª–∏ –ø—Ä–∏ fallback
        if (not semantic_ids_to_use or not paginated_ids) and (not smart_search or fallback_smart_search) and (not theme or fallback_theme_search):
            query += " LIMIT ? OFFSET ?"
            params.extend([limit, offset])

        print(f"üìù Executing query with {len(params)} params")
        cursor.execute(query, params)
        rows = cursor.fetchall()

        projects = []
        for row in rows:
            project_data = dict(row)
            if project_data.get("icon"):
                project_data["icon"] = f"data:image/png;base64,{base64.b64encode(project_data['icon']).decode()}"
            else:
                project_data["icon"] = None
            projects.append(project_data)
        
        print(f"‚úÖ Returning {len(projects)} projects")
        return projects
        
    except sqlite3.Error as e:
        print(f"‚ùå SQL error: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ SQL-–∑–∞–ø—Ä–æ—Å–∞: {e}")
    finally:
        if conn:
            conn.close()

@router.post("/clear-search-cache")
async def clear_search_cache():
    """–û—á–∏—â–∞–µ—Ç –∫—ç—à –ø–æ–∏—Å–∫–∞ (–¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è)"""
    with _cache_lock:
        _search_cache.clear()
    perform_search_once.cache_clear()
    return {"message": "Search cache cleared"}


def levenshtein_distance(s1: str, s2: str) -> int:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏"""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def stem_word(word: str) -> str:
    """–ë–∞–∑–æ–≤–∞—è —Å—Ç–µ–º–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤"""
    if not word or len(word) < 3:
        return word
    
    russian_endings = [
        '–æ–≤', '–µ–≤', '–∏–Ω', '—ã–Ω', '—ã—Ö', '–∏—Ö', '–æ–µ', '–µ–µ', '—ã–µ', '–∏–µ', '–æ–º—É', '–µ–º—É', 
        '—ã–º–∏', '–∏–º–∏', '–æ–º', '–µ–º', '–∞—Ö', '—è—Ö', '–∞–º–∏', '—è–º–∏', '—É—é', '—é—é', '–µ–π', '–æ–π',
        '–∞', '—è', '–æ', '–µ', '–∏', '—ã', '—É', '—é', '—å', '–π', '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏'
    ]
    
    english_endings = [
        'ing', 'ed', 'es', 's', 'ly', 'er', 'est', 'ment', 'ness', 'tion', 'sion'
    ]
    
    for ending in english_endings:
        if word.endswith(ending) and len(word) > len(ending) + 2:
            return word[:-len(ending)]
    
    for ending in russian_endings:
        if word.endswith(ending) and len(word) > len(ending) + 2:
            return word[:-len(ending)]
    
    return word

def normalize_and_stem(text: str) -> Set[str]:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å—Ç–µ–º–º–∏–Ω–≥–æ–º"""
    words = re.findall(r'\b\w{2,}\b', text.lower())
    stemmed_words = set()
    
    for word in words:
        stemmed_words.add(word)
        stemmed = stem_word(word)
        if stemmed != word and len(stemmed) >= 2:
            stemmed_words.add(stemmed)
    
    return stemmed_words

def expand_with_synonyms(word: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç —Å–ª–æ–≤–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
    synonyms = set()
    word_lower = word.lower()
    
    synonyms.add(word_lower)
    
    stemmed = stem_word(word_lower)
    if stemmed != word_lower:
        synonyms.add(stemmed)
    
    # –ò—â–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –≤ –±–∞–∑–µ
    if word_lower in SYNONYM_DB:
        synonyms.update(SYNONYM_DB[word_lower])
    
    if stemmed in SYNONYM_DB:
        synonyms.update(SYNONYM_DB[stemmed])
    
    for key, synonym_list in SYNONYM_DB.items():
        if word_lower in synonym_list or stemmed in synonym_list:
            synonyms.add(key)
            synonyms.update(synonym_list)
    
    return synonyms

def expand_query_with_synonyms(query: str) -> Set[str]:
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏ —Å —É—á–µ—Ç–æ–º —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
    words = re.findall(r'\b\w{2,}\b', query.lower())
    expanded_terms = set()
    
    for word in words:
        synonyms = expand_with_synonyms(word)
        expanded_terms.update(synonyms)
    
    return expanded_terms

def build_search_index(conn):
    """–°—Ç—Ä–æ–∏–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞"""
    global search_index, project_data_cache, ALL_TOKENS
    
    with _index_lock:
        cursor = conn.cursor()
        cursor.execute("SELECT id, name, theme, type, is_premium FROM projects")
        rows = cursor.fetchall()
        
        search_index = {}
        project_data_cache = {}
        all_unique_tokens = set()
        
        print(f"üîç Building search index for {len(rows)} projects")
        
        for row in rows:
            project = dict(row)
            project_id = project['id']
            project_data_cache[project_id] = project
            
            content = f"{project['name']} {project['theme']} {project['type']}".lower()
            
            stemmed_words = normalize_and_stem(content)
            
            enhanced_tokens = set()
            for word in stemmed_words:
                enhanced_tokens.add(word)
                synonyms = expand_with_synonyms(word)
                enhanced_tokens.update(synonyms)
                
                if len(word) > 3:
                    for i in range(len(word) - 2):
                        enhanced_tokens.add(word[i:i+3])
            
            word_count = Counter(enhanced_tokens)
            total_words = len(enhanced_tokens)
            
            search_index[project_id] = {
                'tf': {word: count/total_words for word, count in word_count.items()},
                'content': content,
                'original_words': stemmed_words,
                'all_tokens': set(enhanced_tokens),
                'is_premium': project.get('is_premium', False)
            }        
            all_unique_tokens.update(enhanced_tokens)
        
        ALL_TOKENS = list(all_unique_tokens)
        print(f"üìä Search index built with {len(ALL_TOKENS)} unique tokens")

def find_similar_words_fast(query_word: str, max_distance: int = 2) -> List[str]:
    """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–Ω–¥–µ–∫—Å–∞"""
    similar_words = []
    query_lower = query_word.lower()
    query_stemmed = stem_word(query_lower)
    
    for token in ALL_TOKENS:
        if abs(len(query_lower) - len(token)) > max_distance:
            continue
            
        distance = levenshtein_distance(query_lower, token)
        if distance <= max_distance and distance > 0:  
            max_len = max(len(query_lower), len(token))
            similarity = 1 - (distance / max_len)
            similar_words.append((token, similarity))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
    similar_words.sort(key=lambda x: x[1], reverse=True)
    return [word for word, score in similar_words[:3]]

def spell_aware_semantic_search(query, threshold=0.2, top_k=30):
    """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ–º –ø–æ score"""
    global search_index
    
    print(f"üîç Starting spell-aware search for: '{query}'")
    
    if not search_index:
        print("‚ùå Search index is empty!")
        return []
    
    expanded_terms = expand_query_with_synonyms(query)
    
    if not expanded_terms:
        print("‚ùå No valid terms after expansion")
        return []
    
    original_query_words = re.findall(r'\b\w{2,}\b', query.lower())
    
    use_detailed_spell_check = len(original_query_words) <= 3
    
    similar_words_cache = {}
    if use_detailed_spell_check:
        for query_word in original_query_words:
            similar_words = find_similar_words_fast(query_word)
            if similar_words:
                similar_words_cache[query_word] = similar_words
    
    query_tf = {term: 1.0/len(expanded_terms) for term in expanded_terms}
    
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        project_tokens = project_data['all_tokens']
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (—Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        exact_name_matches = 0
        for term in expanded_terms:
            if term in project_name and any(term == word for word in project_data['original_words']):
                exact_name_matches += 1
        if exact_name_matches > 0:
            similarity += exact_name_matches * 3.0  
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–º–µ
        exact_theme_matches = 0
        for term in expanded_terms:
            if term in project_theme and any(term == word for word in project_data['original_words']):
                exact_theme_matches += 1
        if exact_theme_matches > 0:
            similarity += exact_theme_matches * 2.0  
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        partial_name_matches = 0
        for term in expanded_terms:
            if term in project_name and not any(term == word for word in project_data['original_words']):
                partial_name_matches += 1
        if partial_name_matches > 0:
            similarity += partial_name_matches * 1.5
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 4: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Ç–µ–º–µ
        partial_theme_matches = 0
        for term in expanded_terms:
            if term in project_theme and not any(term == word for word in project_data['original_words']):
                partial_theme_matches += 1
        if partial_theme_matches > 0:
            similarity += partial_theme_matches * 1.0
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 5: –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
        if use_detailed_spell_check and similar_words_cache:
            similar_word_bonus = 0
            for query_word, similar_words in similar_words_cache.items():
                matched_similar = set(similar_words) & project_tokens
                if matched_similar:
                    similar_word_bonus += min(0.5, 0.2 * len(matched_similar))
            similarity += similar_word_bonus
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 6: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
        similarity += min(cosine_sim, 1.0)
        
        # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–µ–º–∏—É–º –ø—Ä–æ–µ–∫—Ç—ã
        if project_info.get('is_premium'):
            similarity += 0.1
        
        if similarity >= threshold:
            similarities.append((project_id, similarity, exact_name_matches, exact_theme_matches))

    # –í–ê–ñ–ù–û: –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–Ω–∏—è
    if similarities:
        similarities.sort(key=lambda x: x[1], reverse=True)
        scores = [score for _, score, _, _ in similarities]
        
        if len(scores) > 5:
            top_score = scores[0]
            
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥: –º–∏–Ω–∏–º—É–º 60% –æ—Ç –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            dynamic_threshold = max(threshold, top_score * 0.60)
            absolute_min_threshold = 0.4  
            final_threshold = min(dynamic_threshold, absolute_min_threshold)
            
            print(f"üéØ Dynamic threshold: {final_threshold:.3f} (top_score: {top_score:.3f})")
            
            filtered_count_before = len(similarities)
            similarities = [
                (pid, score, name_m, theme_m) 
                for pid, score, name_m, theme_m in similarities 
                if score >= final_threshold
            ]
            filtered_count_after = len(similarities)
            
            print(f"üìä Filtered: {filtered_count_before} ‚Üí {filtered_count_after} results")
    
    print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    for pid, score, name_matches, theme_matches in similarities[:5]:
        project_info = project_data_cache.get(pid, {})
        print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}'")
        print(f"      Theme: {project_info.get('theme', 'N/A')}")
        print(f"      Score: {score:.4f} (name_matches: {name_matches}, theme_matches: {theme_matches})")
    
    return [{'id': pid, 'score': score} for pid, score, _, _ in similarities[:top_k]]

ALL_TOKENS = []

def calculate_cosine_similarity(query_tf, doc_tf):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
    all_words = set(query_tf.keys()) | set(doc_tf.keys())
    
    dot_product = 0
    query_magnitude = 0
    doc_magnitude = 0
    
    for word in all_words:
        query_val = query_tf.get(word, 0)
        doc_val = doc_tf.get(word, 0)
        
        dot_product += query_val * doc_val
        query_magnitude += query_val ** 2
        doc_magnitude += doc_val ** 2
    
    if query_magnitude == 0 or doc_magnitude == 0:
        return 0
    
    return dot_product / (math.sqrt(query_magnitude) * math.sqrt(doc_magnitude))

def normalize_search_term(term):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    if not term:
        return ""
    
    term = re.sub(r'\s+', ' ', term.lower()).strip()
    return term

def enhanced_semantic_search(query, threshold=0.01, top_k=20):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    global search_index
    
    print(f"üîç Starting enhanced search for: '{query}'")
    print(f"üîç Threshold: {threshold}, Top K: {top_k}")
    
    if not search_index:
        print("‚ùå Search index is empty!")
        return []
    
    query_lower = query.lower()
    query_words = re.findall(r'\b\w{2,}\b', query_lower)
    
    if not query_words:
        print("‚ùå No valid words in query")
        return []
    
    # print(f"üîç Query words: {query_words}")
    similarities = []
    
    for project_id, project_data in search_index.items():
        similarity = 0
        
        # –°–ø–æ—Å–æ–± 1: –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        partial_matches = find_partial_matches(query_lower, project_data['tf'])
        if partial_matches:
            best_match_score = max(score for _, score in partial_matches)
            similarity = max(similarity, best_match_score)
        
        # –°–ø–æ—Å–æ–± 2: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —Å–ª–æ–≤–∞–º
        if query_words:
            query_tf = {word: 1.0/len(query_words) for word in query_words}
            cosine_sim = calculate_cosine_similarity(query_tf, project_data['tf'])
            similarity = max(similarity, cosine_sim)
        
        project_info = project_data_cache.get(project_id, {})
        project_name = project_info.get('name', '').lower()
        project_theme = project_info.get('theme', '').lower()
        
        for q_word in query_words:
            if q_word in project_name:
                similarity += 0.3
            if q_word in project_theme:
                similarity += 0.2
        
        if similarity >= threshold:
            similarities.append((project_id, similarity))
    
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"üìä Found {len(similarities)} results above threshold {threshold}")
    
    for pid, score in similarities[:5]:
        project_info = project_data_cache.get(pid, {})
        print(f"   üéØ Project {pid}: '{project_info.get('name', 'N/A')}'")
        print(f"      Theme: {project_info.get('theme', 'N/A')}")
        print(f"      Score: {score:.4f}")
    
    return [{'id': pid, 'score': score} for pid, score in similarities[:top_k]]

def refresh_search_index(conn):
    """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    print("üîÑ Forced search index refresh...")
    build_search_index(conn)